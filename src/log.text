【2022.5.13】
尝试运行这个代码
processor.py 是用来将数据集转换为KG的
movie数据集 第一次train  耗时25min 台式机CPU
epoch 0    train auc: 0.9730  f1: 0.9200    eval auc: 0.9681  f1: 0.9141    test auc: 0.9683  f1: 0.9143
epoch 1    train auc: 0.9818  f1: 0.9372    eval auc: 0.9740  f1: 0.9245    test auc: 0.9741  f1: 0.9251
epoch 2    train auc: 0.9881  f1: 0.9506    eval auc: 0.9772  f1: 0.9311    test auc: 0.9773  f1: 0.9313
epoch 3    train auc: 0.9912  f1: 0.9586    eval auc: 0.9775  f1: 0.9315    test auc: 0.9776  f1: 0.9317
epoch 4    train auc: 0.9934  f1: 0.9643    eval auc: 0.9772  f1: 0.9311    test auc: 0.9772  f1: 0.9315
epoch 5    train auc: 0.9947  f1: 0.9684    eval auc: 0.9765  f1: 0.9300    test auc: 0.9765  f1: 0.9302
epoch 6    train auc: 0.9956  f1: 0.9717    eval auc: 0.9759  f1: 0.9297    test auc: 0.9760  f1: 0.9298
epoch 7    train auc: 0.9962  f1: 0.9739    eval auc: 0.9756  f1: 0.9288    test auc: 0.9757  f1: 0.9290
epoch 8    train auc: 0.9965  f1: 0.9755    eval auc: 0.9752  f1: 0.9284    test auc: 0.9753  f1: 0.9287
epoch 9    train auc: 0.9968  f1: 0.9767    eval auc: 0.9748  f1: 0.9277    test auc: 0.9749  f1: 0.9280
time used: 1527 s

music数据集耗时很少
epoch 0    train auc: 0.8530  f1: 0.7879    eval auc: 0.5161  f1: 0.5167    test auc: 0.5028  f1: 0.5002
epoch 1    train auc: 0.8673  f1: 0.7875    eval auc: 0.5965  f1: 0.5700    test auc: 0.5932  f1: 0.5693
epoch 2    train auc: 0.8280  f1: 0.7394    eval auc: 0.6931  f1: 0.6244    test auc: 0.6970  f1: 0.6280
epoch 3    train auc: 0.8375  f1: 0.7242    eval auc: 0.7496  f1: 0.6507    test auc: 0.7516  f1: 0.6535
epoch 4    train auc: 0.8655  f1: 0.7369    eval auc: 0.7754  f1: 0.6658    test auc: 0.7740  f1: 0.6679
epoch 5    train auc: 0.8773  f1: 0.7584    eval auc: 0.7832  f1: 0.6878    test auc: 0.7821  f1: 0.6834
epoch 6    train auc: 0.8852  f1: 0.7679    eval auc: 0.7863  f1: 0.6921    test auc: 0.7856  f1: 0.6904
epoch 7    train auc: 0.8901  f1: 0.7756    eval auc: 0.7869  f1: 0.6955    test auc: 0.7871  f1: 0.6994
epoch 8    train auc: 0.8923  f1: 0.7795    eval auc: 0.7867  f1: 0.6964    test auc: 0.7877  f1: 0.7018
epoch 9    train auc: 0.8950  f1: 0.7848    eval auc: 0.7862  f1: 0.6988    test auc: 0.7885  f1: 0.7052
time used: 13 s

新建自己的仓库 吐了  https://github.com/dingN98/KGCN

.npy 文件是在train的时候生成的  功能未知

【2022.5.14】
准备看论文  今天要看完  12:08结束
计划2：就是将这篇论文的movie知识图谱导入到neo4j里，看看到底啥样子
计划3：理解代码

【2022.5.15】
今天早上成功将 ml-1m 数据导入到 neo4j
导入实体的耗时很短，导入关系的耗时很长（100万条关系，导入了1.7小时）
然后这个知识图谱的问题是，实体的结构太简单（比如movie实体的属性在实体里面，而不是和外部实体有关系，不利于实现论文里的那种信息聚合和多跳传播）
所以，今天的计划
计划1：继续看论文的那个代码，研究下它是怎么建立KG的
计划2：重新建一个知识图谱 kg_movies_2 实体movie里面加入更多 属性实体

尝试理解preprocess.py
1、字典 item_index_old2new 作用是做一个映射，就是将【老的item的id】映射成【新的item的id】，前者是不连续的，后者是连续的。
字典 user_index_old2new同理
2、字典 user_pos_ratings 的作用是保存用户的积极评分【rating>=4】，key是【userid】,value是个集合【set】，包含该用户积极评论过的所有电影的id【movieid】
字典 user_neg_ratings 的作用的保存评分<4的数据
3、ratings_final.txt 保存着 【新的userid】、【新的itemid】、【1或者0】（1代表用户对这个电影的评分>=4，0代表用户没看过这个电影，每个用户的1和0数量一样）
4、item_index2entity_id.txt 的作用是id映射，就是把知识图谱里的【itemid】映射到【连续的itemid】
5、kg.txt是movie知识图谱，三个字段分别是【电影实体id】、【关系】、【其他实体id】

现在可以确定的是，KGCN的作者构建了一个movie知识图谱，其中的【实体】包含以下：【电影id】、【genre】、【actor】、【producer】、【director】、【star】、【language】等等。
其中的【关系】是一种三级结构，典型的实例【film.film.actor】、【film.film.language】、【media_common.creative_work.honors】等等。
然后作者从这个知识图谱里将【电影实体-关系-其他实体】三元组提取出来，存储在kg.txt里，一个实例是【11904	film.film.producer	16954】，
作者对kg.txt做了预处理，就是将文本关系映射成数字，最后结果是32个关系，6036个user，16954个item，102569个实体（包含item）。

【2022.5.16】
data_loader.py
1、构建评分数据集
load_rating() 将ratings_final.txt【user,movie,rating】转换为npy格式（原因未知），然后将ratings_final.npy 按照 6:2:2的比例划分为 train，eval,test（训练集、验证集、测试集）
2、构建知识图谱
load_kg() 将kg_final.txt【head,relation,tail】转换为npy格式，然后构建 一个dict() 叫做kg。这个字典的key是head,然后value是(tail,relation)的二元组；key是tail，value是(head,relation)的二元组。因此，这个kg字典应该是个无向图。
3、构建邻接矩阵
load_adj() 构架俩邻接矩阵：adj_entity 和 adj_relation ，二者都是二维数组【实体数量*邻域采样大小】，元素类型是int64。
数组的行标代表【实体A的id】，然后对于 adj_entity，列代表【实体B的id】，对于adj_relation，列代表【实体A和实体B的关系】

model.py
执行流程
def __init__(self, args, n_user, n_entity, n_relation, adj_entity, adj_relation):
        self._parse_args(args, adj_entity, adj_relation)
        self._build_inputs()
        self._build_model(n_user, n_entity, n_relation)
        self._build_train()

1、_parse_args()  导入了 【实体邻接矩阵】、【关系邻接矩阵】、【迭代次数】、【batch_size】、【向量维度】、【l2_weight】、【lr】、【聚合器】

2、_build_inputs() 在tf里做【user_indices】、【item_indices】、【labels】的占位符

3、_build_model() 主要涉及到三个嵌入矩阵，user_emb_matrix、entity_emb_matrix、relation_emb_matrix 和 衍生矩阵
（1）user的嵌入向量
user_emb_matrix 【n_user * dim】，user_embeddings 【batch_size * dim】 。 user_embeddings是在user_emb_matrix里选取的 batch_size 大小的数据。
self.user_embeddings = tf.nn.embedding_lookup(self.user_emb_matrix, self.user_indices)

【2022.5.17】

（2）通过item_indices 获取 邻域实体和关系
entities, relations = self.get_neighbors(self.item_indices)
（3）将邻域实体和关系 通过 【聚合器】聚合信息
self.item_embeddings, self.aggregators = self.aggregate(entities, relations)
（4）计算模型得分并正则化
self.scores = tf.reduce_sum(self.user_embeddings * self.item_embeddings, axis=1)
self.scores_normalized = tf.sigmoid(self.scores)

4、_build_train()
（1）基础loss  base_loss 计算标签labels和得分scores的交叉熵
（2）l2_loss 好复杂  和 聚合器 aggregator 的weights也有关  以后研究下
（3）最终的loss等于三个loss的和
self.loss = self.base_loss + self.l2_weight * self.l2_loss
（4）优化器
self.optimizer = tf.train.AdamOptimizer(self.lr).minimize(self.loss)

5、train()
sess.run([self.optimizer, self.loss], feed_dict)

6、eval()评价指标
（1）auc
auc = roc_auc_score(y_true=labels, y_score=scores)
AUC的本质含义反映的是对于任意一对正负例样本,模型将正样本预测为正例的可能性 大于 将负例预测为正例的可能性的 概率
（2）f1
f1 = f1_score(y_true=labels, y_pred=scores)
F1-Measure是根据准确率Precision和召回率Recall二者给出的一个综合的评价指标，具体定义如下：F1 = 2rp / ( r +p )，其中r为recall，p为precision.

train.py
1、载入data
通过data_loader.py获取data ： n_user, n_item, n_entity, n_relation, train_data, eval_data, test_data, adj_entity, adj_relation
2、KGCN模型初始化
model = KGCN(args, n_user, n_entity, n_relation, adj_entity, adj_relation)
3、topK评价指标设置
user_list, train_record, test_record, item_set, k_list = topk_settings(show_topk, train_data, test_data, n_item)
4、正式训练
在每个epoch下做以下工作：
（1）将训练集打乱 shuffle
（2）循环：在训练集中选取 batch_size行 的数据计算，剩余的一小部分数据直接丢弃
调用model.train()函数训练
（3）评价训练结果【训练集、验证集、测试集】
调用ctr_eval()函数评价，得到auc和f1
（4）如果开启了topK评价指标
通过topk_eval()函数计算

5、一些子函数
（1）topk_settings() 调用 get_user_record()
训练集 train_record 来自 train_data 里label为1或者0的数据；测试集 test_record 来自 test_data 里label为1的样本。
意味着，train_record包含正样本和负样本，test_record只包含正样本。二者的结构都是dict()，其中key是userid，value是itemid的set。
函数返回 return user_list, train_record, test_record, item_set, k_list，
其中，user_list 是 train_record 和 test_record 的user的交集，长度上限100，
item_set = set(list(range(n_item)))，
k_list = [1, 2, 5, 10, 20, 50, 100]  作用未知。

（2）topk_eval()
这个函数的一开始的两行揭示了 k_list的作用，就是：需要多个topK的评判，即top1,top2,top5,top10,top20,top50,top100，最后再取平均值。
【循环】对于user_list的每个user，做以下操作：
1）将要输入到模型里的数据：test_item_list = list(item_set - train_record[user])，就是推测用户user对他喜欢的item之外的其他的item的评分，
2）输出dict()   item_score_map[item] = score
3)将item_score_map 按照得分进行降序排序，得到 item_score_pair_sorted
4）item_sorted = [i[0] for i in item_score_pair_sorted]  只保留itemid
5)计算准确率 precision 和召回率 recall
首先计算【topk推荐的item】和【用户喜欢的item】的交集：hit_num = len(set(item_sorted[:k]) & test_record[user])
然后，
precision_list[k].append(hit_num / k)
recall_list[k].append(hit_num / len(test_record[user]))
最后求平均，得到最终的俩指标。

（3）get_feed_dict()
用于获取 data 中 batch_size 大小的数据

（4）ctr_eval()
调用 model.eval()计算 f1 和 auc
首先是循环，计算一个 batch_size 里的 f1 和 auc，然后将结果加入到 list 里，最后对 list 里的数据求平均。

aggregators.py
1、初始化父类【Aggregator】
def __init__(self, batch_size, dim, dropout, act, name):
        if not name:
            layer = self.__class__.__name__.lower()
            name = layer + '_' + str(get_layer_id(layer))
        self.name = name
        self.dropout = dropout
        self.act = act
        self.batch_size = batch_size
        self.dim = dim

2、信息聚合核心函数
（1）邻域节点们的信息聚合  _mix_neighbor_vectors()
【权重 = 用户嵌入 * 关系嵌入】
user_relation_scores = tf.reduce_mean(user_embeddings * neighbor_relations, axis=-1)
正则化
user_relation_scores_normalized = tf.nn.softmax(user_relation_scores, dim=-1)
扩展
user_relation_scores_normalized = tf.expand_dims(user_relation_scores_normalized, axis=-1)
【邻域实体信息 = 权重 * 邻域实体嵌入】
neighbors_aggregated = tf.reduce_mean(user_relation_scores_normalized * neighbor_vectors, axis=2)

（2）【自己的嵌入】和【邻域节点们传入的嵌入】的聚合  _call()
三种聚合器的代码差别
【SumAggregator】俩个嵌入【相加】
output = tf.reshape(self_vectors + neighbors_agg, [-1, self.dim])
【ConcatAggregator】两个嵌入【拼接】
output = tf.concat([self_vectors, neighbors_agg], axis=-1)
【NeighborAggregator】直接放弃【自己的嵌入】
output = tf.reshape(neighbors_agg, [-1, self.dim])



【看完代码后的疑问】
1、user、item、relation 三者的embedding嵌入到底是啥？
